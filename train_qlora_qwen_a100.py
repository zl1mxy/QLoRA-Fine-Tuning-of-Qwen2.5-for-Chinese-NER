import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import load_from_disk
import os
import wandb

def setup_a100_environment():
    """A100ä¸“ç”¨ç¯å¢ƒè®¾ç½®"""
    print("=" * 70)
    print("ğŸš€ A100 40GB + Qwen2.5-7B QLoRA å¾®è°ƒè®­ç»ƒ")
    print("=" * 70)
    
    # æ£€æŸ¥A100
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"âœ… GPU: {gpu_name}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        # A100ç‰¹æœ‰ä¼˜åŒ–
        if "A100" in gpu_name:
            print("ğŸ¯ A100æ£€æµ‹åˆ°ï¼Œå¯ç”¨ä¼˜åŒ–é…ç½®:")
            print("   - ä½¿ç”¨TF32ç²¾åº¦")
            print("   - å¢å¤§batch size")
            print("   - å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹")
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
    
    return "cuda"

def load_model_for_a100(model_name="Qwen/Qwen2.5-7B-Instruct"):
    """A100ä¼˜åŒ–ç‰ˆæ¨¡å‹åŠ è½½"""
    print(f"\nğŸ”§ åŠ è½½æ¨¡å‹: {model_name}")
    
    # A100å¯ä½¿ç”¨æ›´é«˜æ•ˆçš„é‡åŒ–
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,  # A100æ”¯æŒbfloat16
        bnb_4bit_use_double_quant=True,
    )
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="right",
        use_fast=False
    )
    
    # è®¾ç½®pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½æ¨¡å‹ï¼ˆA100å¯ç”¨bfloat16åŠ é€Ÿï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        attn_implementation="eager",
    quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,  # A100ä¼˜åŒ–
    )
    
    model = prepare_model_for_kbit_training(model)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   æ¨¡å‹å¤§å°: {model.num_parameters() / 1e9:.2f}B å‚æ•°")
    print(f"   é‡åŒ–: 4-bit NF4 + bfloat16")
    print(f"   Flash Attention: å·²å¯ç”¨")
    
    return model, tokenizer

def setup_a100_lora(model):
    """A100ä¼˜åŒ–çš„LoRAé…ç½®"""
    print("\nğŸ¯ é…ç½®LoRAå‚æ•°ï¼ˆA100ä¼˜åŒ–ï¼‰")
    
    lora_config = LoraConfig(
        r=32,  # A100æ˜¾å­˜å¤§ï¼Œå¯å¢åŠ ç§©
        lora_alpha=64,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                       "gate_proj", "up_proj", "down_proj",
                       "lm_head"],  # å¢åŠ lm_head
        lora_dropout=0.05,  # å‡å°‘dropout
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({(trainable_params/total_params)*100:.2f}%)")
    print(f"ğŸ¯ æ€»å‚æ•°: {total_params:,}")
    
    return model

def train_on_a100():
    """åœ¨A100ä¸Šè®­ç»ƒ"""
    device = setup_a100_environment()
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    try:
        dataset = load_from_disk("ner_instruction_dataset")
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(dataset['train']):,} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(dataset['validation']):,} æ ·æœ¬")
        
        # æ˜¾ç¤ºæ•°æ®å¤§å°
        train_size = len(dataset['train'])
        if train_size < 10000:
            print(f"âš ï¸  æ•°æ®é‡è¾ƒå° ({train_size} samples)ï¼Œå¯å¢åŠ è®­ç»ƒè½®æ•°")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        print("è¯·å…ˆè¿è¡Œ: python data_preprocess.py")
        return None
    
    # 2. åŠ è½½æ¨¡å‹å’Œtokenizer
    model, tokenizer = load_model_for_a100()
    
    # 3. è®¾ç½®LoRA
    model = setup_a100_lora(model)
    
    # 4. A100ä¼˜åŒ–è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./qwen2.5-7b-ner-qlora-a100",
        num_train_epochs=5,  # A100å¯å¢åŠ è½®æ•°
        per_device_train_batch_size=2,  # A100å¯å¢å¤§batch size
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=8,  # å‡å°‘æ¢¯åº¦ç´¯ç§¯
        warmup_ratio=0.03,  # ä½¿ç”¨æ¯”ä¾‹è€Œéå›ºå®šæ­¥æ•°
        logging_steps=20,
        eval_steps=100,
        save_steps=200,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        learning_rate=3e-4,  # A100å¯å¢å¤§å­¦ä¹ ç‡
        fp16=False,  # A100ç”¨bf16
        bf16=True,  # A100æ”¯æŒbfloat16
        optim="paged_adamw_32bit",  # 32bitä¼˜åŒ–å™¨
        max_grad_norm=0.5,
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        report_to="tensorboard",
        logging_dir="./a100_logs",
        save_total_limit=5,  # å¤šä¿å­˜å‡ ä¸ªæ£€æŸ¥ç‚¹
        push_to_hub=False,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        dataloader_num_workers=4,  # A100å¯å¢åŠ worker
        remove_unused_columns=False,
        group_by_length=True,  # æŒ‰é•¿åº¦åˆ†ç»„ï¼Œæé«˜æ•ˆç‡
    )
    
    # 5. æ ¼å¼åŒ–å‡½æ•°
    def format_for_a100(example):
        """A100ä¼˜åŒ–çš„æ ¼å¼åŒ–"""
        text = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
è¯·è¯†åˆ«ä»¥ä¸‹æ–‡æœ¬ä¸­çš„å®ä½“ï¼š

{example['input']}

æ ¼å¼ï¼š{{å®ä½“ç±»å‹: å®ä½“}}<|im_end|>
<|im_start|>assistant
{example['output']}<|im_end|>"""
        return text
    
    # 6. åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        max_seq_length=2048,  # A100å¯å¤„ç†æ›´é•¿åºåˆ—
        packing=True,  # å¼€å¯packingæé«˜æ•ˆç‡
        formatting_func=format_for_a100,
        dataset_text_field="text",
    )
    
    # 7. æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print("\nğŸš€ A100è®­ç»ƒé…ç½®:")
    print(f"   Batch size: {training_args.per_device_train_batch_size}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {training_args.gradient_accumulation_steps}")
    print(f"   æœ‰æ•ˆæ‰¹æ¬¡: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"   å­¦ä¹ ç‡: {training_args.learning_rate}")
    print(f"   è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
        # print(f"   åºåˆ—é•¿åº¦: {trainer.sequence_length}")
    print(f"   ç²¾åº¦: {'bfloat16' if training_args.bf16 else 'float16'}")
    
    # 8. å¼€å§‹è®­ç»ƒ
    print("\nâ³ å¼€å§‹è®­ç»ƒ...")
    train_result = trainer.train()
    
    # 9. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model("./qwen2.5-7b-ner-qlora-a100-final")
    tokenizer.save_pretrained("./qwen2.5-7b-ner-qlora-a100-final")
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åˆ°: ./qwen2.5-7b-ner-qlora-a100-final")
    
    return trainer

def quick_test_a100():
    """å¿«é€Ÿæµ‹è¯•A100æ€§èƒ½"""
    print("\nâš¡ A100å¿«é€Ÿæ€§èƒ½æµ‹è¯•...")
    
    # æµ‹è¯•çŸ©é˜µè¿ç®—é€Ÿåº¦
    device = torch.device("cuda")
    
    # å¤§çŸ©é˜µä¹˜æ³•æµ‹è¯•
    size = 8192
    a = torch.randn(size, size, device=device, dtype=torch.bfloat16)
    b = torch.randn(size, size, device=device, dtype=torch.bfloat16)
    
    torch.cuda.synchronize()
    import time
    start = time.time()
    
    for _ in range(10):
        c = torch.matmul(a, b)
    
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    print(f"âœ… A100æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print(f"   8192x8192çŸ©é˜µ10æ¬¡ä¹˜æ³•: {elapsed:.3f}ç§’")
    print(f"   å¹³å‡æ¯æ¬¡: {elapsed/10:.3f}ç§’")
    
    # æ˜¾å­˜æµ‹è¯•
    print(f"\nğŸ’¾ æ˜¾å­˜æµ‹è¯•:")
    print(f"   æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print(f"   å·²ç”¨æ˜¾å­˜: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"   ç¼“å­˜æ˜¾å­˜: {torch.cuda.memory_reserved() / 1e9:.2f} GB")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        quick_test_a100()
    else:
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        quick_test_a100()
        
        # å¼€å§‹è®­ç»ƒ
        trainer = train_on_a100()
        
        if trainer is not None:
            # è®­ç»ƒåå¿«é€Ÿè¯„ä¼°
            print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼Œå¼€å§‹è¯„ä¼°...")
            os.system("python inference_a100.py")