import pandas as pd
import json
import re
from datasets import Dataset, DatasetDict
import numpy as np

def load_ner_data():
    """åŠ è½½NERæ•°æ®é›†"""
    print("åŠ è½½NERæ•°æ®é›†...")
    
    # è®­ç»ƒé›†
    train_df = pd.read_parquet("ner_output/ner_data.parquet")
    print(f"è®­ç»ƒé›†: {len(train_df):,} è¡Œ")
    
    # éªŒè¯é›†
    valid_df = pd.read_parquet("ner_datasets/valid_ner/valid_ner.parquet")
    print(f"éªŒè¯é›†: {len(valid_df):,} è¡Œ")
    
    return train_df, valid_df

def format_instruction_data(row):
    """æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®"""
    # æå–instructionä¸­çš„æ ¸å¿ƒæŒ‡ä»¤
    instruction = row['instruction']
    if isinstance(instruction, str) and len(instruction) > 500:
        # æˆªæ–­è¿‡é•¿çš„æŒ‡ä»¤ï¼Œä¿ç•™æ ¸å¿ƒéƒ¨åˆ†
        lines = instruction.split('\n')
        core_instruction = lines[0] if lines else instruction[:200]
        instruction = core_instruction
    
    # æ„å»ºå®Œæ•´çš„prompt
    prompt = f"""è¯·è¯†åˆ«å¹¶æ ‡æ³¨ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ä¸­çš„å‘½åå®ä½“ï¼ˆåŒ…æ‹¬äººåã€åœ°åã€æ—¶é—´ã€ç»„ç»‡åã€å…¬å¸åã€äº§å“åç­‰ï¼‰ã€‚

æ–‡æœ¬ï¼š
{row['input']}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æ ‡æ³¨ï¼š
{{å®ä½“ç±»å‹: å®ä½“æ–‡æœ¬}}

è¯·ç›´æ¥è¾“å‡ºæ ‡æ³¨ç»“æœï¼š"""
    
    # è·å–outputï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
    if isinstance(row['output'], str) and '{{' in row['output'] and '}}' in row['output']:
        response = row['output']
    else:
        # å¦‚æœæ²¡æœ‰æ­£ç¡®çš„æ ¼å¼ï¼Œä½¿ç”¨inputä½œä¸ºå ä½
        response = f"æœªæ‰¾åˆ°å®ä½“æ ‡æ³¨ã€‚åŸå§‹æ–‡æœ¬ï¼š{row['input'][:100]}..."
    
    return {
        "instruction": instruction[:300] if isinstance(instruction, str) else "",
        "input": row['input'],
        "output": response,
        "text": f"### æŒ‡ä»¤ï¼š{instruction}\n\n### è¾“å…¥ï¼š{row['input']}\n\n### è¾“å‡ºï¼š{response}"
    }

def create_huggingface_dataset():
    """åˆ›å»ºHugging Faceæ•°æ®é›†"""
    train_df, valid_df = load_ner_data()
    
    print("\næ ¼å¼åŒ–è®­ç»ƒæ•°æ®...")
    train_data = []
    for _, row in train_df.iterrows():
        try:
            formatted = format_instruction_data(row)
            train_data.append(formatted)
        except Exception as e:
            print(f"æ ¼å¼åŒ–è®­ç»ƒæ•°æ®æ—¶å‡ºé”™: {e}")
            continue
    
    print("æ ¼å¼åŒ–éªŒè¯æ•°æ®...")
    valid_data = []
    for _, row in valid_df.iterrows():
        try:
            formatted = format_instruction_data(row)
            valid_data.append(formatted)
        except Exception as e:
            print(f"æ ¼å¼åŒ–éªŒè¯æ•°æ®æ—¶å‡ºé”™: {e}")
            continue
    
    # è½¬æ¢ä¸ºDataset
    train_dataset = Dataset.from_list(train_data)
    valid_dataset = Dataset.from_list(valid_data)
    
    # åˆ›å»ºDatasetDict
    dataset_dict = DatasetDict({
        "train": train_dataset,
        "validation": valid_dataset
    })
    
    print(f"\nâœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(valid_dataset):,} æ ·æœ¬")
    
    # ä¿å­˜åˆ°ç£ç›˜
    dataset_dict.save_to_disk("ner_instruction_dataset")
    print("ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜åˆ°: ner_instruction_dataset")
    
    # ä¿å­˜ä¸ºJSONLä¾›æ£€æŸ¥
    with open("ner_dataset_sample.jsonl", "w", encoding="utf-8") as f:
        for i in range(min(10, len(train_data))):
            f.write(json.dumps(train_data[i], ensure_ascii=False) + "\n")
    print("ğŸ“ æ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ°: ner_dataset_sample.jsonl")
    
    return dataset_dict

def analyze_entity_distribution(dataset):
    """åˆ†æå®ä½“åˆ†å¸ƒ"""
    print("\nğŸ” åˆ†æå®ä½“ç±»å‹åˆ†å¸ƒ...")
    
    entity_types = []
    for sample in dataset["train"].select(range(min(1000, len(dataset["train"])))):
        output = sample["output"]
        if isinstance(output, str):
            matches = re.findall(r'\{\{([^:]+):', output)
            entity_types.extend(matches)
    
    if entity_types:
        from collections import Counter
        type_counter = Counter(entity_types)
        
        print("å®ä½“ç±»å‹åˆ†å¸ƒ (å‰20):")
        for entity_type, count in type_counter.most_common(20):
            print(f"  {entity_type:20s}: {count:4} æ¬¡")
    
    return entity_types

if __name__ == "__main__":
    dataset = create_huggingface_dataset()
    analyze_entity_distribution(dataset)